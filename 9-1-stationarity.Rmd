---
title: "9. ARIMA models"
author: "9.1 Stationary and differencing"
date: "OTexts.org/fpp3/"
classoption: aspectratio=169
titlepage: fpp3title.png
titlecolor: fpp3red
toc: false
output:
  binb::monash:
    colortheme: monashwhite
    fig_width: 7.5
    fig_height: 2.8
    keep_tex: no
    includes:
      in_header: fpp3header.tex
---

```{r setup, include=FALSE}
source("setup.R")
library(patchwork)
```

## Stationarity

\begin{block}{Definition}
If $\{y_t\}$ is a \orange{stationary time series}, then for all $s$, the distribution of $(y_t,\dots,y_{t+s})$ does not depend on $t$.
\end{block}\pause

A \orange{stationary series} is:

* roughly horizontal
* constant variance
* no patterns predictable in the long-term


## Stationarity

\begin{block}{Definition}
If $\{y_t\}$ is a \orange{stationary time series}, then for all $s$, the distribution of $(y_t,\dots,y_{t+s})$ does not depend on $t$.
\end{block}\pause

A \orange{stationary series} is:

* roughly horizontal
* constant variance
* no patterns predictable in the long-term

## Stationary or not

```{r stationary, echo=FALSE, warning=FALSE, fig.height=4.6, fig.width=10}
p1 <- gafa_stock |>
  filter(Symbol == "GOOG", year(Date) == 2015) |>
  autoplot(Close) +
  labs(subtitle = "(a) Google closing price", x = "Day", y = " $US")

p2 <- gafa_stock |>
  filter(Symbol == "GOOG", year(Date) == 2015) |>
  autoplot(difference(Close)) +
  labs(subtitle = "(b) Change in google price", x = "Day", y = "$US")

p3 <- as_tsibble(fma::strikes) |>
  autoplot(value) +
  labs(subtitle = "(c) Strikes: US",
       y = "Number of strikes",
       x = "Year")

p4 <- as_tsibble(fma::hsales) |>
  autoplot(value) +
  labs(subtitle = "(d) House sales: US",
       y = "Number of houses",
       x = "Month")

p5 <- as_tsibble(fma::eggs) |>
  autoplot(value) +
  labs(subtitle = "(e) Egg prices: US",
       y = "$US (constant prices)",
       x = "Year")

p6 <- aus_livestock |>
  filter(State == "Victoria", Animal == "Pigs") |>
  autoplot(Count) +
  labs(subtitle = "(f) Pigs slaughtered: Victoria, Australia",
       y = "Number of pigs",
       x="Month")

p7 <- pelt |>
  autoplot(Lynx) +
  labs(subtitle = "(g) Lynx trapped: Canada",
       y = "Number of lynx",
       x = "Year")

p8 <- aus_production |>
  filter(year(Quarter) %in% 1991:1995) |>
  autoplot(Beer) +
  labs(subtitle = "(h) Beer production: Australia",
       y = "Megalitres",
       x = "Quarter")

p9 <- aus_production |>
  autoplot(Gas) +
  labs(subtitle = "(i) Gas production: Australia",
       y = "Petajoules",
       x = "Quarter")

(p1 | p2 | p3) / (p4 | p5 | p6) / (p7 | p8 | p9)
```





## Stationarity

\begin{block}{Definition}
If $\{y_t\}$ is a stationary time series, then for all $s$, the distribution of $(y_t,\dots,y_{t+s})$ does not depend on $t$.
\end{block}\pause\vspace*{0.4cm}

* Transformations help to \orange{stabilize the variance}.
* For ARIMA modelling, we also need to \orange{stabilize the mean}.

## Non-stationarity in the mean
\alert{Identifying non-stationary series}

* time plot.
* The ACF of stationary data drops to zero relatively quickly
* The ACF of non-stationary data decreases slowly.
* For non-stationary data, the value of $r_1$ is often large and positive.

## Example: Google stock price

```{r}
google_2018 <- gafa_stock |>
  filter(Symbol == "GOOG", year(Date) == 2018)
```

## Example: Google stock price

```{r}
google_2018 |>
  autoplot(Close) +
  labs(y = "Closing stock price ($USD)")
```

## Example: Google stock price

```{r}
google_2018 |>
  ACF(Close) |>
  autoplot()
```

## Example: Google stock price

```{r}
google_2018 |>
  autoplot(difference(Close)) +
  labs(y = "Change in Google closing stock price ($USD)")
```

## Example: Google stock price

```{r}
google_2018 |>
  ACF(difference(Close)) |>
  autoplot()
```

## Differencing

* Differencing helps to \orange{stabilize the mean}.
* The differenced series is the \orange{change} between each observation in the original series: $y'_t = y_t - y_{t-1}$.
* The differenced series will have \orange{only $T-1$ values} since it is not possible to calculate a difference $y_1'$ for the first observation.

## Example: Google stock price 

* The differences are the \orange{day-to-day} changes.
* Now the series looks just like a white noise series:
    * No autocorrelations outside the 95% limits.
    * Large Ljung-Box p-value.  
* \orange{Conclusion:} The daily change in the Google stock price is essentially a random amount uncorrelated with previous days.


## Random walk model

If differenced series is white noise with zero mean:

\begin{block}{}
\centerline{$y_t-y_{t-1}=\varepsilon_t$ \hspace{0.4cm} or \hspace{0.4cm} $y_t=y_{t-1}+\varepsilon_t$}
\end{block}\vspace*{-0.3cm}
where $\varepsilon_t \sim NID(0,\sigma^2)$.

* Very widely used for non-stationary data.
* This is the model behind the \orange{naïve method}.
* Random walks typically have:
    * long periods of apparent trends up or down
    * Sudden/unpredictable changes in direction
* Forecast are equal to the last observation (naïve)
    * future movements up or down are equally likely.

## Random walk with drift model

If differenced series is white noise with non-zero mean:

\begin{block}{}
\centerline{$y_t-y_{t-1}=c+\varepsilon_t$ \hspace{0.4cm} or \hspace{0.4cm} $y_t=c+y_{t-1}+\varepsilon_t$}
\end{block}\vspace*{-0.3cm}
where $\varepsilon_t \sim NID(0,\sigma^2)$.

* $c$ is the \orange{average change} between consecutive observations.
* If $c>0$, $y_t$ will tend to drift upwards and vice versa.
* This is the model behind the \orange{drift method}.

\vspace*{10cm}

## Second-order differencing

Occasionally the differenced data will not appear stationary and it may be necessary to difference the data a second time:\pause
\begin{align*}
y''_{t} & = y'_{t} - y'_{t - 1} \\
        & = (y_t - y_{t-1}) - (y_{t-1}-y_{t-2})\\
        & = y_t - 2y_{t-1} +y_{t-2}.
\end{align*}\pause

* $y_t''$ will have $T-2$ values.
* In practice, it is almost never necessary to go beyond second-order differences.

## Seasonal differencing

\fontsize{14}{14}\sf

A \orange{seasonal difference} is the difference between an observation and the corresponding observation from the previous year.
$$
 y'_t = y_t - y_{t-m}
$$
where $m=$ number of seasons.\pause

* For monthly data $m=12$.
* For quarterly data $m=4$.
* Seasonally differenced series will have \orange{$T-m$} obs.\pause
  
\vspace{0.4cm}

If seasonally differenced data is white noise it implies: 
\begin{block}{}
\centerline{$y_t-y_{t-m}=\varepsilon_t$ \hspace{0.4cm} or \hspace{0.4cm} $y_t=y_{t-m}+\varepsilon_t$}
\end{block}

  * The model behind the \orange{seasonal naïve} method.

## Antidiabetic drug sales

```{r, echo=TRUE}
a10 <- PBS |>
  filter(ATC2 == "A10") |>
  summarise(Cost = sum(Cost) / 1e6)
```

## Antidiabetic drug sales

```{r, echo=TRUE}
a10 |> autoplot(
  Cost
)
```

## Antidiabetic drug sales

```{r, echo=TRUE}
a10 |> autoplot(
  log(Cost)
)
```

## Antidiabetic drug sales

```{r, echo=TRUE}
a10 |> autoplot(
  log(Cost) |> difference(12)
)
```

## Corticosteroid drug sales

```{r, echo=TRUE}
h02 <- PBS |>
  filter(ATC2 == "H02") |>
  summarise(Cost = sum(Cost) / 1e6)
```

## Corticosteroid drug sales

```{r, echo=TRUE}
h02 |> autoplot(
  Cost
)
```

## Corticosteroid drug sales

```{r, echo=TRUE}
h02 |> autoplot(
  log(Cost)
)
```

## Corticosteroid drug sales

```{r, echo=TRUE}
h02 |> autoplot(
  log(Cost) |> difference(12)
)
```

## Corticosteroid drug sales

```{r, echo=TRUE}
h02 |> autoplot(
  log(Cost) |> difference(12) |> difference(1)
)
```

## Corticosteroid drug sales

* Seasonally differenced series is closer to being stationary.
* Remaining non-stationarity can be removed with further first difference.

If $y'_t = y_t - y_{t-12}$ denotes seasonally differenced series, then twice-differenced series is

\begin{block}{}
\begin{align*}
y^*_t &= y'_t - y'_{t-1} \\
      &= (y_t - y_{t-12}) - (y_{t-1} - y_{t-13}) \\
      &= y_t - y_{t-1} - y_{t-12} + y_{t-13}\: .
\end{align*}
\end{block}\vspace*{10cm}

## Seasonal differencing

When both seasonal and first differences are applied\dots\pause

* it makes no difference which is done first---the result will be the same.
* If seasonality is strong, we recommend that seasonal differencing be done first because sometimes the resulting series will be stationary and there will be no need for further first difference.\pause

It is important that if differencing is used, the differences are interpretable.

## Interpretation of differencing

* first differences are the change between \orange{one observation and the next};
* seasonal differences are the change between \orange{one year to the next}.
\pause

But taking lag 3 differences for yearly data, for example, results in a model which cannot be sensibly interpreted.


## Unit root tests

Statistical tests to determine the required order of differencing.

  1. Augmented Dickey Fuller test: null hypothesis is that the data are \orange{non-stationary} and non-seasonal.
  2. Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test: null hypothesis is that the data are \orange{stationary} and non-seasonal.
  3. Other tests available for seasonal data.

\only<2>{\begin{textblock}{4}(10.2,4)
\begin{alertblock}{}
H$_0$: non-stationary
\end{alertblock}
\end{textblock}}

\only<2>{\begin{textblock}{3}(12,5.4)
\begin{alertblock}{}
H$_0$: stationary
\end{alertblock}
\end{textblock}}


## KPSS test
\fontsize{9}{10}\sf

```{r, echo=TRUE}
google_2018 %>%
   features(Close, unitroot_kpss)
```

\pause

```{r, echo=TRUE}
google_2018 %>%
  features(Close, unitroot_ndiffs)
```

## Automatically selecting differences

\fontsize{13}{14}\sf

STL decomposition: $y_t = T_t+S_t+R_t$

Seasonal strength $F_s = \max\big(0, 1-\frac{\text{Var}(R_t)}{\text{Var}(S_t+R_t)}\big)$

If $F_s > 0.64$, do one seasonal difference.

\fontsize{10}{11}\sf

```{r, echo=TRUE}
h02 %>% mutate(log_sales = log(Cost)) %>%
  features(log_sales, list(unitroot_nsdiffs, feat_stl))
```

## Automatically selecting differences
\fontsize{10}{11}\sf

```{r, echo=TRUE}
h02 %>% mutate(log_sales = log(Cost)) %>%
  features(log_sales, unitroot_nsdiffs)
h02 %>% mutate(d_log_sales = difference(log(Cost), 12)) %>%
  features(d_log_sales, unitroot_ndiffs)
```